{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "> **Warning!** Please run `01_cleaning.ipynb` first if you haven't already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functions.constants import BM_NAME, STARTDATE, ENDDATE, N_THRESHOLD_BPS,DATA_DIR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded active returns from ./../data/SP500_active_returns.csv\n",
      "Loaded active returns thresholded from ./../data/SP500_active_returns_thresholded_100bps.csv\n"
     ]
    }
   ],
   "source": [
    "active_returns_path = DATA_DIR + BM_NAME + \"_active_returns.csv\"\n",
    "active_returns = pd.read_csv(active_returns_path, index_col=0, parse_dates=True)\n",
    "print(\"Loaded active returns from\", active_returns_path)\n",
    "active_returns_thresholded_path = DATA_DIR + BM_NAME + \"_active_returns_thresholded_\" + str(N_THRESHOLD_BPS) + \"bps.csv\"\n",
    "active_returns_thresholded = pd.read_csv(active_returns_thresholded_path, index_col=0, parse_dates=True)\n",
    "print(\"Loaded active returns thresholded from\", active_returns_thresholded_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previewing the thresholded data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Col Name           || True Count || False Count\n",
      "active_returns_1b  || 408.0      || 2044.0\n",
      "active_returns_1w  || 800.0      || 1652.0\n",
      "active_returns_1m  || 806.0      || 1646.0\n",
      "active_returns_1q  || 805.0      || 1647.0\n",
      "active_returns_1y  || 801.0      || 1651.0\n"
     ]
    }
   ],
   "source": [
    "TEST_TICKER = \"GS UN\" # Goldman Sachs--also try \"AAPL UW\" and \"JPM UN\"\n",
    "TEST_PERIODS = [\"1b\", \"1w\", \"1m\", \"1q\", \"1y\"]\n",
    "period_columns = [\"active_returns_\" + period for period in TEST_PERIODS]\n",
    "test_ticker_df = active_returns_thresholded[active_returns_thresholded[\"Ticker\"] == TEST_TICKER]\n",
    "test_ticker_df\n",
    "#True and False counts for each period \n",
    "true_counts = test_ticker_df[period_columns].sum()\n",
    "false_counts = len(test_ticker_df) - true_counts\n",
    "print(\"Col Name           || True Count || False Count\")\n",
    "for col in period_columns:\n",
    "    print(f\"{col:<18} || {true_counts[col]:<10} || {false_counts[col]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumb Model that forecasts +1 if previous period's active_returns_1b was +1, 0 if it was 0. Specific to GS UN\n",
      "Precision: 0.19852941176470587\n",
      "Recall: 0.19852941176470587\n",
      "F1 Score: 0.19852941176470587\n",
      "Accuracy: 0.7331701346389229\n",
      "True Positive: 81\n",
      "False Positive: 327\n",
      "True Negative: 1716\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter for test period\n",
    "TEST_PERIOD = \"1b\"  # This can be set to different periods like \"1b\", \"1w\", \"1m\", \"1q\", \"1y\"\n",
    "shift_bizdays = 1\n",
    "#for our given TEST_TICKER let us construct a simple strategy that forecasts 1 if yesterday's active_returns_1b was 1, 0 if it was 0\n",
    "test_ticker_df = active_returns_thresholded[active_returns_thresholded[\"Ticker\"] == TEST_TICKER]\n",
    "test_ticker_df = test_ticker_df[[\"Ticker\", \"Date\", f\"active_returns_{TEST_PERIOD}\"]] \n",
    "#soort in ascending by Ticker first then Date\n",
    "test_ticker_df = test_ticker_df.sort_values([\"Ticker\", \"Date\"])\n",
    "test_ticker_df[f\"active_returns_{TEST_PERIOD}_forecast_dumb\"] = test_ticker_df[f\"active_returns_{TEST_PERIOD}\"].shift(shift_bizdays)\n",
    "#drop row where forecast column is NaN\n",
    "test_ticker_df = test_ticker_df.dropna()\n",
    "#measure precision and recall of this dumb model, get f1 score and accuracy\n",
    "true_positive = len(test_ticker_df[(test_ticker_df[f\"active_returns_{TEST_PERIOD}\"] == 1) & (test_ticker_df[f\"active_returns_{TEST_PERIOD}_forecast_dumb\"] == 1)])\n",
    "false_positive = len(test_ticker_df[(test_ticker_df[f\"active_returns_{TEST_PERIOD}\"] == 0) & (test_ticker_df[f\"active_returns_{TEST_PERIOD}_forecast_dumb\"] == 1)])\n",
    "true_negative = len(test_ticker_df[(test_ticker_df[f\"active_returns_{TEST_PERIOD}\"] == 0) & (test_ticker_df[f\"active_returns_{TEST_PERIOD}_forecast_dumb\"] == 0)])\n",
    "false_negative = len(test_ticker_df[(test_ticker_df[f\"active_returns_{TEST_PERIOD}\"] == 1) & (test_ticker_df[f\"active_returns_{TEST_PERIOD}_forecast_dumb\"] == 0)])\n",
    "precision = true_positive / (true_positive + false_positive)\n",
    "recall = true_positive / (true_positive + false_negative)\n",
    "\n",
    "print(f\"Dumb Momentum Model that forecasts +1 if previous period's active_returns_{TEST_PERIOD} was +1, 0 if it was 0. Specific to\", TEST_TICKER)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", 2 * precision * recall / (precision + recall))\n",
    "print(\"Accuracy:\", (true_positive + true_negative) / (true_positive + true_negative + false_positive + false_negative))\n",
    "print(\"True Positive:\", true_positive)\n",
    "print(\"False Positive:\", false_positive)\n",
    "print(\"True Negative:\", true_negative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PERIOD = \"1b\"  # This can be set to different periods like \"1b\", \"1w\", \"1m\", \"1q\", \"1y\"\n",
    "shift_bizdays = 1\n",
    "# Initialize counters for the global confusion matrix\n",
    "global_true_positive = 0\n",
    "global_false_positive = 0\n",
    "global_true_negative = 0\n",
    "global_false_negative = 0\n",
    "\n",
    "for ticker in active_returns_thresholded[\"Ticker\"].unique():\n",
    "    # Filter the data for the current ticker\n",
    "    ticker_df = active_returns_thresholded[active_returns_thresholded[\"Ticker\"] == ticker]\n",
    "    ticker_df = ticker_df[[\"Ticker\", \"Date\", f\"active_returns_{TEST_PERIOD}\"]]  # Use the TEST_PERIOD here\n",
    "    \n",
    "    # Sort by Ticker and Date\n",
    "    ticker_df = ticker_df.sort_values([\"Ticker\", \"Date\"])\n",
    "    \n",
    "    # Create forecast column based on the TEST_PERIOD\n",
    "    ticker_df[f\"active_returns_{TEST_PERIOD}_forecast_dumb\"] = ticker_df[f\"active_returns_{TEST_PERIOD}\"].shift(shift_bizdays)\n",
    "    \n",
    "    # Drop rows with NaN in forecast column\n",
    "    ticker_df = ticker_df.dropna()\n",
    "    \n",
    "    global_true_positive += len(ticker_df[(ticker_df[f\"active_returns_{TEST_PERIOD}\"] == 1) & (ticker_df[f\"active_returns_{TEST_PERIOD}_forecast_dumb\"] == 1)])\n",
    "    global_false_positive += len(ticker_df[(ticker_df[f\"active_returns_{TEST_PERIOD}\"] == 0) & (ticker_df[f\"active_returns_{TEST_PERIOD}_forecast_dumb\"] == 1)])\n",
    "    global_true_negative += len(ticker_df[(ticker_df[f\"active_returns_{TEST_PERIOD}\"] == 0) & (ticker_df[f\"active_returns_{TEST_PERIOD}_forecast_dumb\"] == 0)])\n",
    "    global_false_negative += len(ticker_df[(ticker_df[f\"active_returns_{TEST_PERIOD}\"] == 1) & (ticker_df[f\"active_returns_{TEST_PERIOD}_forecast_dumb\"] == 0)])\n",
    "\n",
    "# Calculate overall precision, recall, F1 score, and accuracy\n",
    "precision = global_true_positive / (global_true_positive + global_false_positive) if (global_true_positive + global_false_positive) != 0 else 0\n",
    "recall = global_true_positive / (global_true_positive + global_false_negative) if (global_true_positive + global_false_negative) != 0 else 0\n",
    "f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) != 0 else 0\n",
    "accuracy = (global_true_positive + global_true_negative) / (global_true_positive + global_true_negative + global_false_positive + global_false_negative)\n",
    "\n",
    "# Print the overall metrics\n",
    "print(f\"Dumb Momentum Model Forecasting Across All Tickers for period {TEST_PERIOD}:\")\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1_score)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"True Positive:\", global_true_positive)\n",
    "print(\"False Positive:\", global_false_positive)\n",
    "print(\"True Negative:\", global_true_negative)\n",
    "print(\"False Negative:\", global_false_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
